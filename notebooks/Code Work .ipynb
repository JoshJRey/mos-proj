{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Setting Up The Environment\n",
    "\n",
    "The first step is to run the setup file in the virtual environment set up by Alastair, as shown below. \n",
    "This is another notebook in the workspaces that loads the correct set of python libraries and packages that ensure that all scripts equally.\n",
    "In cell 8 of setup make sure to change :\n",
    "\n",
    "> ag1k_dir = '/Users/Utente/Documents/UROP2018/'\n",
    "\n",
    "to your own folder (in which you have vector_ops). \n",
    "<br>Also change the output directory location in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".container {\n",
       "    width: 100%;\n",
       "}\n",
       "div#notebook {\n",
       "    padding-top: 1em;\n",
       "}\n",
       "#header-container {\n",
       "    display: none;\n",
       "}\n",
       "#header-bar {\n",
       "    display: none;\n",
       "}\n",
       "#maintoolbar {\n",
       "    display: none;\n",
       "}\n",
       "#menubar-container {\n",
       "    position: fixed;\n",
       "    margin-top: 0;\n",
       "}\n",
       "#site {\n",
       "    height: auto !important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run setup.ipynb\n",
    "output_dir = Path('/Users/Utente/Documents/UROP2018/Outputfiles/vector/observatory/analysis/107-sfs-data-request-imperial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Extract Outgroup Data\n",
    "\n",
    "The outgroups that are available are : Arab, Quad, Meru, Mela, Chri, Epir.\n",
    "<br> Only one sequence is available for Chri and Epir\n",
    "<br> Arab, Quad, Meru, Mela, contain the sequences of several individuals. \n",
    "<br> Arab has 12 individuals sequences, Quad and Meru have 10 individuals each and Mela has 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def load_outgroup_alleles(seqid, species):\n",
    "\n",
    "    if species in {'chri', 'epir'}:\n",
    "        variants_fn = (\n",
    "            '/Users/Utente/Documents/UROP2018/phase1.AR3/agc/%s_fake_cnvrt_sort.vcf.gz.vcfnp_cache/variants.%s.npy'\n",
    "            % (species, seqid)\n",
    "        )\n",
    "        variants = np.load(variants_fn)\n",
    "        pos = variants['POS']\n",
    "        ref = variants['REF']\n",
    "        alt = variants['ALT']\n",
    "        \n",
    "        outgroup_allele = np.where(alt == b'.', ref, alt)\n",
    "        \n",
    "        # fill out to length of chromosome\n",
    "        gens = phase2_ar1.genome_agamp3\n",
    "        out = np.empty(len(gens[seqid]), dtype='S1')\n",
    "        out[:] = b'N'\n",
    "        out[pos - 1] = outgroup_allele\n",
    "        \n",
    "        # Emma edit - creating multiple arrays to make data homogenous \n",
    "        pop_alt = np.zeros((len(gens[seqid]),2), dtype = 'U')\n",
    "        pop_ac = np.zeros((len(gens[seqid]),2), dtype = int)\n",
    "        pop_ac[:,0]=1\n",
    "        \n",
    "        pop_alt[:,0]=out\n",
    "        \n",
    "        return pop_ac, pop_alt.astype('U')\n",
    "        \n",
    "    else:\n",
    "\n",
    "        callset_fn = (\n",
    "            '/Users/Utente/Documents/UROP2018/phase1.AR3/agc/UnifiedGenotyper/%s_ref_ug_vqsr_cnvrt_sort.h5'\n",
    "            % species\n",
    "        )\n",
    "        callset = h5py.File(callset_fn, mode='r')\n",
    "        \n",
    "        # compute allele counts\n",
    "        g = allel.GenotypeDaskArray(callset[seqid]['calldata/genotype'])\n",
    "        ac = g.count_alleles(max_allele=1).compute()\n",
    "        an = ac.sum(axis=1).max()\n",
    "\n",
    "        # obtain helper arrays\n",
    "        variants = callset[seqid]['variants']\n",
    "        pos = variants['POS'][:]\n",
    "        ref = variants['REF'][:]\n",
    "        alt = variants['ALT'][:]\n",
    "        \n",
    "        # don't call allele unless it's fixed\n",
    "        outgroup_allele = np.empty_like(ref)\n",
    "        outgroup_allele[:] = b'N'\n",
    "        loc_ref = ac[:, 0] == an\n",
    "        loc_alt = ac[:, 1] == an\n",
    "        outgroup_allele[loc_ref] = ref[loc_ref]\n",
    "        outgroup_allele[loc_alt] = alt[loc_alt]\n",
    "        \n",
    "        # fill out to length of chromosome\n",
    "        gens = phase2_ar1.genome_agamp3\n",
    "        out = np.empty(len(gens[seqid]), dtype='S1')\n",
    "        out[:] = b'N'\n",
    "        out[pos - 1] = outgroup_allele\n",
    "        \n",
    "        # Emma edit - creating multiple arrays to make data homogenous \n",
    "        pop_alt = np.empty((len(gens[seqid]),2), dtype='S1')\n",
    "        pop_ac = np.empty((len(gens[seqid]),2), dtype= int)\n",
    "\n",
    "        pop_alt[:] = b'N'\n",
    "        pop_ac[:] = 0\n",
    "\n",
    "        pop_alt[pos-1,0] = ref\n",
    "        pop_alt[pos-1,1] = alt\n",
    "        pop_ac[pos-1,:] = ac\n",
    "\n",
    "        #return out.astype('U'), ac_all, ref_all.astype('U'), alt_all.astype('U')\n",
    "        return pop_ac, pop_alt.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EDIT THIS - NOT EDITED OUT NOW\n",
    "The 'Emma Edit' is an addition to Alistair's original function, that puts the data in the format shown below, and that is compatible to the other functions given below. To extract the various sequences from some of the outgroups use the code above that has been commented out. The function was changed from Alistair's array, that outputted a single output sequence per outgroup, to one that outputs two arrays, the population counts and the allele base array. Making the output data format equal to the population arrays allows for more generic Basic Stat divergence and pi functions to be created.\n",
    "<br> These output arrays:\n",
    "\n",
    "- __pop_alt__ = Size equals length of genome, with two columns. At each site gives counts of the number of sequences with the reference base (first column), and number of sequences with the alternate base (second column). \n",
    "- __pop_ac__ = Size equals length of genome, with two columns. At each site gives reference base if one is available (first column), and alternate base if it exists(second column). \n",
    "\n",
    "Load the data as shown in the example below, for the various outgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqid = '3L'\n",
    "#to extract a single sequence\n",
    "a_ac  , a_alt   = load_outgroup_alleles(seqid, 'arab')\n",
    "q_ac  , q_alt   = load_outgroup_alleles(seqid, 'quad')\n",
    "mer_ac, mer_alt = load_outgroup_alleles(seqid, 'meru')\n",
    "mel_ac, mel_alt = load_outgroup_alleles(seqid, 'mela')\n",
    "c_ac  , c_alt   = load_outgroup_alleles(seqid, 'chri')\n",
    "e_ac  , e_alt   = load_outgroup_alleles(seqid, 'epir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Site Annotations to Extract\n",
    "\n",
    "Some data pertaining to the bases is useful, and used to generate the output data. Three key ones are shown below.\n",
    "\n",
    "__seq_cls__\n",
    "<br>Differentiates between the various sequence classes below. UTR refers to the untranslated regions on the ends of genes. CDS refers to gene coding sequences. UPSTREAM and DOWNSTREAM refer to the sequences on the ends of and between genes. \n",
    "<br>CLS_UPSTREAM = 1\n",
    "<br>CLS_DOWNSTREAM = 2\n",
    "<br>CLS_5UTR = 3\n",
    "<br>CLS_3UTR = 4\n",
    "<br>CLS_CDS_FIRST = 5\n",
    "<br>CLS_CDS_MID = 6\n",
    "<br>CLS_CDS_LAST = 7\n",
    "<br>CLS_INTRON_FIRST = 8\n",
    "<br>CLS_INTRON_MID = 9\n",
    "<br>CLS_INTRON_LAST = 10\n",
    "<br>Refer to Alistair's build-sequence-classes.ipynb for greater detail.\n",
    "\n",
    "__codon_position__ \n",
    "<br>Denotes the codon position (0,1,2) for every exon base location. (-1) otherwise. \n",
    "<br>Refer to Alistair's build-codon-degeneracy.ipynb for greater detail.\n",
    "\n",
    "__codon_degeneracy__\n",
    "<br>Denotes the degeneracy of each codon base. (1) denotes a zero-fold site, (4) denotes a four-fold site. \n",
    "<br>Refer to Alistair's build-codon-degeneracy.ipynb for greater detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_cls = zarr.open_group(str(output_dir / 'seq_cls.zarr.zip'))[seqid][:]\n",
    "codon_position = zarr.open_group(str(output_dir / 'codon_position.zarr.zip'))[seqid][:]\n",
    "codon_degeneracy = zarr.open_group(str(output_dir / 'codon_degeneracy.zarr.zip'))[seqid][:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EDIT - MOVE THE CODE BELOW TO THE BUILD POPULATION SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callset = phase2_ar1.callset\n",
    "alt = phase2_ar1.callset[seqid]['variants']['ALT'][:]\n",
    "pos = phase2_ar1.callset[seqid]['variants']['POS'][:]\n",
    "allele_counts = phase2_ar1.allele_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Filters Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Genome accessibility: \n",
    "An important filter that will be used is genome accessibility. Below are relevant exerpts from _Genetic diversity of the African malaria vector Anopheles gambiae_ Supplementary Information 3.4 Genome Accessibility \n",
    "\n",
    "The following metrics were computed from alignments for the 765 individual specimens that passed all sample QC criteria defined above: \n",
    "\n",
    " - __No Coverage__: The percentage of individuals with zero coverage at the given position. \n",
    " - __Low Coverage__: The percentage of individuals with low coverage at the given position (less than half modal coverage for the whole chromosome arm).\n",
    " - __High Coverage__: The percentage of individuals with high coverage at the given position (more than twice modal coverage for the whole chromosome). \n",
    " - __Ambiguous Alignment__: The percentage of individuals with more than 10% of reads ambiguously aligned (mapping quality zero) at the given position. \n",
    " - __Low Mapping Quality__: The percentage of individuals with average mapping quality less than 30 at the given position. \n",
    " - __Low Read Pairing__: The percentage of individuals with less than 90% of reads aligned as part of a proper pair (mate pair is aligned in expected orientation within a reasonable distance) at the given position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt',\n",
       " 'chrom',\n",
       " 'coverage',\n",
       " 'coverage_mq0',\n",
       " 'filter_dust',\n",
       " 'filter_high_coverage',\n",
       " 'filter_high_mq0',\n",
       " 'filter_low_coverage',\n",
       " 'filter_low_mq',\n",
       " 'filter_n',\n",
       " 'filter_no_coverage',\n",
       " 'high_coverage',\n",
       " 'high_mq0',\n",
       " 'is_accessible',\n",
       " 'low_coverage',\n",
       " 'low_mq',\n",
       " 'low_pairing',\n",
       " 'no_coverage',\n",
       " 'pos',\n",
       " 'ref',\n",
       " 'ref_masked',\n",
       " 'ref_n',\n",
       " 'repeat_dust',\n",
       " 'repeat_repeatmask',\n",
       " 'repeat_trf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqid = '3L'\n",
    "list(phase2_ar1.accessibility[seqid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each location the numbers below shows the individuals for which there is no coverage. The total number is 1142 as there are 571 individuals across all 16 populations. The same format is available for all the accessibility-related arrays that pertains to the chromosome sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_ar1.accessibility[seqid]['no_coverage'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important one from the list above is __is_accessible__. The final decision was to classify a reference genome position as accessible if and only if all of the following conditions were met: \n",
    " - __Not repeat masked by DUST__ \n",
    " - __No Coverage <= 0.2%__ (at most 1 individual had zero coverage) \n",
    " - __Ambiguous Alignment <= 0.2%__ (at most 1 individual had ambiguous alignments) \n",
    " - __High Coverage <= 2%__ (15 individuals) \n",
    " (to resolve sites where total coverage across all individuals might appear normal but some individuals have excessively low coverage while others have excessively high coverage.)\n",
    " - __Low Coverage <= 10%__ (76 individuals)\n",
    " - __Low Mapping Quality <= 10%__ (76 individuals) \n",
    " \n",
    " <br> For more detailed information or explanations refer to the Supplementary Information of the 1000 genome paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs = phase2_ar1.accessibility[seqid]['is_accessible'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Filtering (Filter 2.0 as called in the data files)\n",
    "\n",
    "A whole range of filters have been applied to the data in time. The final filter to be applied has data meet the following conditions: \n",
    "\n",
    "<br> For (BFgam, BFcol, KE) the following must be true for all three populations: \n",
    " - The sites must be accessible. \n",
    " - The sites must be biallelic within each population.\n",
    " - If the site is polymorphic, it must have the total number of individuals be accounted for in each population.\n",
    " \n",
    "<br> For each outgroup (arab, quad, meru, mela, chri, epir) the following must be true for each outgroup:\n",
    " - There must be at least one valid outgroup sequence at each site. \n",
    " \n",
    "<br>Additionally, filter for four-fold, zero-fold, total filtered individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below builds the filter, and the line below it runs it.\n",
    "<br> The data to be loaded before building the function:\n",
    " - pop_ac for (BFgam, BFcol, KE) and (arab, quad, meru, mela, chri, epir).\n",
    " - codon_degeneracy, as shown in section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_filter_tot():\n",
    "    #biallelic for all populations\n",
    "    BFg_bia = (BFg_ac[:,2]<=0)\n",
    "    BFc_bia = (BFc_ac[:,2]<=0)\n",
    "    KE_bia = (KE_ac[:,2]<=0)\n",
    "    bia_tot = BFg_bia & BFc_bia & KE_bia\n",
    "    \n",
    "    #All individuals present\n",
    "    BFg_full = (BFg_b[:,0]==t4)\n",
    "    BFc_full = (BFc_b[:,0]==t5)\n",
    "    KE_full = (KE_b[:,0]==t14)\n",
    "    full_tot = BFg_full & BFc_full & KE_full\n",
    "                \n",
    "    #Valid Outgroup Sites\n",
    "    sum_a = np.sum(a_ac, axis=1)>= 1\n",
    "    filter_a = sum_a & np.invert((a_ac[:,0]>0) & (a_alt[:,0]=='N'))\n",
    "    sum_q = np.sum(q_ac, axis=1)>= 1\n",
    "    filter_q = sum_q & np.invert((q_ac[:,0]>0) & (q_alt[:,0] =='N'))\n",
    "    sum_mer = np.sum(mer_ac, axis=1)>= 1\n",
    "    filter_mer = sum_mer & np.invert((mer_ac[:,0]>0) & (mer_alt[:,0] =='N'))                           \n",
    "    sum_mel = np.sum(mel_ac, axis=1)>= 1\n",
    "    filter_mel = sum_mel & np.invert((mel_ac[:,0]>0) & (mel_alt[:,0] =='N'))\n",
    "    filter_c = (c_alt[:,0]!='N')\n",
    "    filter_e = (e_alt[:,0]!='N')\n",
    "    outgroup_full = filter_a & filter_q & filter_mer & filter_mel & filter_c & filter_e\n",
    "    \n",
    "    #Accessibility \n",
    "    acs = phase2_ar1.accessibility[seqid]['is_accessible'][:]\n",
    "    \n",
    "    #Total Filter\n",
    "    filter_new_tot = acs & bia_tot & full_tot & outgroup_full\n",
    "    \n",
    "    #codon degeneracy - 0-fold and 4-fold filters\n",
    "    fourfold = codon_degeneracy==4\n",
    "    zerofold = codon_degeneracy==1\n",
    "    filter_new_0 = filter_new_tot & zerofold \n",
    "    filter_new_4 = filter_new_tot & fourfold\n",
    "    \n",
    "    return filter_new_tot, filter_new_0, filter_new_4\n",
    "\n",
    "new_filter_tot, new_filter_0, new_filter_4 = new_filter_tot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend building the filter once, then saving it as a zarr file and accessing it as such in separate jupyter documents. This makes for much easier access, and means the filter needs to only be built once (it is a very computationally heavy function. The code below shows how to save a simple zarr array, and access it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the zarr arrays from numpy arrays new_filter_tot, new_filter_0, new_filter_4\n",
    "\n",
    "n_fil_tot = zarr.array(new_filter_tot)\n",
    "n_fil_0 = zarr.array(new_filter_0)\n",
    "n_fil_4 = zarr.array(new_filter_4)\n",
    "\n",
    "#saving the zarr arrays to the output path \n",
    "\n",
    "zarr.save(os.path.join(output_dir, 'n_fil_tot_3R.zarr'), n_fil_tot)\n",
    "zarr.save(os.path.join(output_dir, 'n_fil_0_3R.zarr'), n_fil_0)\n",
    "zarr.save(os.path.join(output_dir, 'n_fil_4_3R.zarr'), n_fil_4)\n",
    "\n",
    "#Accessing the filters from a different file/ a different time\n",
    "\n",
    "new_filter_tot = zarr.load(os.path.join(output_dir, 'n_fil_tot_3L.zarr') )\n",
    "new_filter_0 =  zarr.load(os.path.join(output_dir, 'n_fil_0_3L.zarr')   )\n",
    "new_filter_4 =  zarr.load(os.path.join(output_dir, 'n_fil_4_3L.zarr')   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Building Population Matrices for (BFgam, BFcol, KE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pop_ac and pop_alt are built for populations BFgam, BFcol, KE in the same format as they are with outgroup data in section 2. The only difference is that there is no more concept of a 'reference' and 'alternate', but the columns are ordered according to the frequency of the allele. There are also four columns, due the data allowing for more than two alternate alleles.\n",
    "\n",
    "- __pop_alt__ = Size equals length of genome, with four columns. At each site gives counts of the number of sequences with the most common allele (first column) to least common allele (fourth column), if these alternate alleles exist. \n",
    "- __pop_ac__ = Size equals length of genome, with four columns. At each site gives base of most common allele (first column) to least common allele (fourth column), if these alleles exist. \n",
    "\n",
    "The columns bases and counts map each other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are meant to be run all together.\n",
    "\n",
    "In the first cell the genome is loaded and re-shaped. 'a', 'c', 'g', 't' are changed to 'A','C','G','T'. The SNP data is put in a suitable format for the next cell, combining reference and alternate alleles.\n",
    "<br> In the second cell the counts are loaded. They are paired with the allele data, and reshuffled in order of frequency, with highest frequency alleles going in the first column, second highest in the second column and so on. Alleles that occur zero times are deleted. Intermediary matrices include:\n",
    " - __altsa__ = (matrix) contains the bases individuals of that population have at that location in the chromosome, ordered by their frequency. Only contain SNP data.\n",
    " - __aca__ = (matrix) contains the numbers of individuals that have the base in the corresponding position in altsa. Only contain SNP data.\n",
    " - __b__ = (array) the total number of individuals accounted for in that population at that location. Only contain SNP data.\n",
    " - __t__ = (integer) total number of individuals of that population.\n",
    "\n",
    "In the third cell populations are built by calling on the function just described __build_mc__. All fifteen populations are shown, but only (BFgam, BFcol, KE) need be built.\n",
    "<br> In the fourth cell the SNP data is re-integrated with the reference sequence to build the final population __pop_ac__ and __pop_alt__. \n",
    "<br> In the third cell populations are built by calling on the function just described __build_extended_mc__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = phase2_ar1.genome_agamp3[seqid][:]\n",
    "genome_a = np.zeros((len(genome),1), dtype = '|S1')\n",
    "for i in range (0,len(genome)):\n",
    "    genome_a[i]=genome[i]\n",
    "genome = genome_a\n",
    "\n",
    "#turning lowercase into uppercase\n",
    "ref = genome[:,0].astype('U')\n",
    "ref[ref=='a']='A'\n",
    "ref[ref=='c']='C'\n",
    "ref[ref=='g']='G'\n",
    "ref[ref=='t']='T'\n",
    "ref[ref=='n']='N'\n",
    "\n",
    "\n",
    "#bringing together all the alternates\n",
    "refv = ref[pos-1]\n",
    "refv = np.reshape(refv, (-1,1))\n",
    "alts_mat = np.concatenate((refv,alt),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mc(pop):\n",
    "    \n",
    "    alts = deepcopy(alts_mat)\n",
    "    \n",
    "    ac = allel.AlleleCountsArray(allele_counts[seqid][pop])\n",
    "    totinds = np.max(ac[:,0])\n",
    "    #print(totinds)\n",
    "\n",
    "\n",
    "    #making all values that don't map to an alternate base -1\n",
    "    ac1 = ac.astype(int) \n",
    "    ac1[alts=='']=-1\n",
    "\n",
    "    #resort!\n",
    "    indices = np.flip(ac1.argsort(),axis = 1)\n",
    "    row = np.reshape(np.array( np.linspace(0,len(ac)-1,len(ac)), dtype = int),(-1,1))\n",
    "    table = np.concatenate((row,row,row,row),axis=1)\n",
    "    atuple = (table,indices)\n",
    "    altsa = alts[atuple]\n",
    "    aca = ac1[atuple]\n",
    "\n",
    "    #delete alleles that occur zero times\n",
    "    altsa[aca==0] = ''\n",
    "    aca[aca==0]=-1\n",
    "\n",
    "    b = deepcopy(aca)\n",
    "    b[aca==-1]=0\n",
    "    b = np.sum(b,axis= 1)\n",
    "    \n",
    "    return altsa, aca, b, totinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seem to be missing GNcol, which on map has 1 female individual but here seems to have 4. either map mistake, different data, or couldn't identify three of the individuals by gender? seems improbable...\n",
    "p1a, p1b, p1c, t1 = build_mc('GM')\n",
    "p2a, p2b, p2c, t2 = build_mc('GW')\n",
    "p3a, p3b, p3c, t3 = build_mc('GNgam')\n",
    "p4a, p4b, p4c, t4 = build_mc('BFgam')\n",
    "p5a, p5b, p5c, t5 = build_mc('BFcol')\n",
    "p6a, p6b, p6c, t6 = build_mc('CIcol')\n",
    "p7a, p7b, p7c, t7 = build_mc('GHgam')\n",
    "p8a, p8b, p8c, t8 = build_mc('GHcol')\n",
    "p9a, p9b, p9c, t9 = build_mc('GQgam')\n",
    "p10a, p10b, p10c, t10 = build_mc('GAgam')\n",
    "p11a, p11b, p11c, t11 = build_mc('CMgam')\n",
    "p12a, p12b, p12c, t12 = build_mc('AOcol')\n",
    "p13a, p13b, p13c, t13 = build_mc('UGgam')\n",
    "p14a, p14b, p14c, t14 = build_mc('KE')\n",
    "p15a, p15b, p15c, t15 = build_mc('FRgam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_extended_mc(altsa, aca, b, totinds):\n",
    "    \n",
    "    ext_ac = np.ones((len(genome),4), dtype =int)*totinds\n",
    "    ext_ac[pos-1, 0] = np.reshape (aca[:,0], (-1,1))\n",
    "    ext_ac[pos-1, 1] = np.reshape (aca[:,1], (-1,1))\n",
    "    ext_ac[pos-1, 2] = np.reshape (aca[:,2], (-1,1))\n",
    "    ext_ac[pos-1, 3] = np.reshape (aca[:,3], (-1,1))\n",
    "    \n",
    "    ext_alt = np.ones((len(genome),4), dtype =int)*totinds\n",
    "    ext_alt[pos-1, 0] = np.reshape (altsa[:,0], (-1,1))\n",
    "    ext_alt[pos-1, 1] = np.reshape (altsa[:,1], (-1,1))\n",
    "    ext_alt[pos-1, 2] = np.reshape (altsa[:,2], (-1,1))\n",
    "    ext_alt[pos-1, 3] = np.reshape (altsa[:,3], (-1,1))\n",
    "    \n",
    "    ext_b = np.ones((len(genome),1), dtype = int)*(totinds) \n",
    "    ext_b[pos-1] = np.reshape(b,(-1,1))\n",
    "\n",
    "    #pos is shared between every population for a given chromosome\n",
    "    pos1 = np.linspace(1,len(genome),len(genome), dtype = int)\n",
    "    pos1 = np.reshape (pos1, (-1,1))\n",
    "    \n",
    "    return ext_ac, ext_alt, ext_b, pos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFg_ac, BFg_alt, BFg_b = build_extended_mc(p4a, p4b, p4c, t4)\n",
    "BFc_ac, BFc_alt, BFc_b = build_extended_mc(p5a, p5b, p5c, t5)\n",
    "KE_ac, KE_alt, KE_b = build_extended_mc(p14a, p14b, p14c, t14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 The 'Basic Stats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Calculating Pi within populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix __pop_ac__ , an output from sections 2 and 5 , is inputted into the function below to calculate pi within each population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_cal(pop_ac):\n",
    "    pop_ac[pop_ac==(-1)]=0\n",
    "    pi_array = allel.mean_pairwise_difference(pop_ac)\n",
    "    return pi_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pi array is built for a population, and values are found by finding the mean of the array as filtered by either __new_filter_tot__, __new_filter_0__ and __new_filter_4__ as shown in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFg_pi = pi_cal(BFg_ac)\n",
    "np.mean(BFg_pi[new_filter_4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Divergence Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting all outgroup and population data into __pop_ac__ and __pop_alt__ has the advantage of being able to use very generalizable functions. The one below requires inputs __pop_ac__ and __pop_alt__ from any two outgroup or Gambiae populations (populations a and b) to give an array of divergences __div__. To reduce the amount of values stored, this is collapsed within the function to output __div1__, which gives the overall diverges values of sites filtered with __new_filter_tot__, __new_filter_0__ and __new_filter_4__.\n",
    "\n",
    "* ADD BETTER DESCRIPTION OF COLLAPSE * \n",
    "\n",
    "The divergence calculation is given below and inplemented in __divving__.\n",
    "\n",
    "divergence = 1 - [(f'A', p1) * (f 'A' ,p2)  + (f'C', p1) * (f 'C' ,p2)  + (f'G', p1) * (f 'G' ,p2)]\n",
    "<br> f'A', p1  refers to the frequency of base A at that specific site in population 1. p1 and p2 are populations 1 and 2. \n",
    "<br> f'A', f'C', f'G', f'T' are the frequencies of bases A,C,G,T respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divving(pop_a_ac, pop_a_alt, pop_b_ac, pop_b_alt):\n",
    "    \n",
    "    pop_a_ac[pop_a_ac==-1]=0\n",
    "    pop_b_ac[pop_b_ac==-1]=0\n",
    "    \n",
    "    div = np.zeros((len(genome),1), dtype = float)\n",
    "    pop_b_ac1 = pop_b_ac/np.reshape(np.sum(pop_b_ac, axis = 1), (-1,1))\n",
    "    pop_a_ac1 = pop_a_ac/np.reshape(np.sum(pop_a_ac, axis = 1), (-1,1))\n",
    "    \n",
    "    div[pop_b_alt[:,0]==pop_a_alt[:,0]]=np.reshape(pop_b_ac1[pop_b_alt[:,0]==pop_a_alt[:,0],0], (-1,1)) *np.reshape((pop_a_ac1[pop_b_alt[:,0]==pop_a_alt[:,0],0]),(-1,1))\n",
    "    div[pop_b_alt[:,0]==pop_a_alt[:,1]]=np.reshape(pop_b_ac1[pop_b_alt[:,0]==pop_a_alt[:,1],0], (-1,1)) *np.reshape((pop_a_ac1[pop_b_alt[:,0]==pop_a_alt[:,1],1]),(-1,1))\n",
    "    div[pop_b_alt[:,1]==pop_a_alt[:,0]]= div[pop_b_alt[:,1]==pop_a_alt[:,0]] + np.reshape(pop_b_ac1[pop_b_alt[:,1]==pop_a_alt[:,0],1], (-1,1)) *np.reshape((pop_a_ac1[pop_b_alt[:,1]==pop_a_alt[:,0],0]),(-1,1))\n",
    "    div[pop_b_alt[:,1]==pop_a_alt[:,1]]= div[pop_b_alt[:,1]==pop_a_alt[:,1]] + np.reshape(pop_b_ac1[pop_b_alt[:,1]==pop_a_alt[:,1],1], (-1,1)) *np.reshape((pop_a_ac1[pop_b_alt[:,1]==pop_a_alt[:,1],1]),(-1,1))\n",
    "    \n",
    "    print('doing...')\n",
    "    div = 1 - div\n",
    "    div1 = [np.mean(div[new_filter_0]),np.mean(div[new_filter_0]),np.mean(div[new_filter_4])]\n",
    "    \n",
    "    #return div\n",
    "    return div1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below merely display building divergences and displaying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_BFg_a     =  divving(BFg_ac    , BFg_alt    , a_ac     , a_alt      )\n",
    "div_BFg_q     =  divving(BFg_ac    , BFg_alt    , q_ac     , q_alt      )\n",
    "div_BFg_mer   =  divving(BFg_ac    , BFg_alt    , mer_ac   , mer_alt    )\n",
    "div_BFg_mel   =  divving(BFg_ac    , BFg_alt    , mel_ac   , mel_alt    )\n",
    "div_BFg_c     =  divving(BFg_ac    , BFg_alt    , c_ac     , c_alt      )\n",
    "div_BFg_e     =  divving(BFg_ac    , BFg_alt    , e_ac     , e_alt      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(['BFg', 'arab', div_BFg_a   ])\n",
    "print(['BFg', 'quad', div_BFg_q   ])\n",
    "print(['BFg', 'meru', div_BFg_mer ])\n",
    "print(['BFg', 'mela', div_BFg_mel ])\n",
    "print(['BFg', 'chri', div_BFg_c   ])\n",
    "print(['BFg', 'epir', div_BFg_e   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Shared Polymorphisms\n",
    "\n",
    "The inputs to the function below are __pop_ac__ for a population a and population b. \n",
    "The outputs are:\n",
    " - __shar__: an array that gives a site a value 1 if it is a site with a shared polymorphism, or 0 if it is not. The definiton of a shared polymorphism is that the same two (and only those two) alleles exist in both populations, although they may have different frequencies.\n",
    " - __descriptor__: a more expanded array that gives the value 1 if both populations have the same monomorphic site, the value 2 for shared polymorphic sites, the value 3 for mixed polymorphic sites, the value 4 for polymorphic sites that completely differ in the alleles that constitute them (only one of the populations need have a polymorphic site), and value 5 for different monomorphic sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_poly(pop_a_alt, pop_b_alt):\n",
    "    \n",
    "    pop_a_alt = pop_a_alt[:,0:2]\n",
    "    pop_b_alt = pop_b_alt[:,0:2]\n",
    "    \n",
    "    pop_a_alt[pop_a_alt=='N']=''\n",
    "    pop_a_alt[pop_a_alt=='.']=''\n",
    "    pop_b_alt[pop_b_alt=='N']=''\n",
    "    pop_b_alt[pop_b_alt=='.']=''\n",
    "    \n",
    "    pop_b_altf = np.flip(pop_b_alt, axis=1)\n",
    "\n",
    "    shar = np.zeros((len(genome)), dtype = bool)\n",
    "    shar[((pop_a_alt==pop_b_alt)[:,0]& (pop_a_alt==pop_b_alt)[:,1]) & (pop_a_alt[:,1]!='')] = 1\n",
    "    shar[((pop_a_alt==pop_b_altf)[:,0]& (pop_a_alt==pop_b_altf)[:,1]) & (pop_a_alt[:,1]!='')] = 1\n",
    "    \n",
    "    #shar1 = [np.mean(shar[new_filter_tot]) , np.mean(shar[new_filter_0]) , np.mean(shar[new_filter_4])]\n",
    "    print('doing...')\n",
    "    \n",
    "    descriptor = np.zeros((len(genome)), dtype = int)\n",
    "    \n",
    "    #Fixed Similar\n",
    "    descriptor[((pop_a_alt==pop_b_alt)[:,0]& (pop_a_alt==pop_b_alt)[:,1]) & (pop_a_alt[:,1]=='')] = 1\n",
    "    # shared polymorphisms\n",
    "    descriptor[shar == 1] = 2\n",
    "    #Polymorphic different\n",
    "    i1 = ((pop_a_alt[:,1]!=pop_b_alt[:,0]) & (pop_a_alt[:,0]!=pop_b_alt[:,1]))\n",
    "    i2 = (pop_a_alt[:,1]=='')\n",
    "    descriptor[((pop_a_alt[:,0]!=pop_b_alt[:,0]) & (pop_a_alt[:,0]!=pop_b_alt[:,1])) & np.any([i1,i2], axis =0)] = 3\n",
    "    #Fixed Different\n",
    "    descriptor[((pop_a_alt!=pop_b_alt)[:,0]& (pop_a_alt==pop_b_alt)[:,1]) & (pop_a_alt[:,1]=='')] = 4\n",
    "    \n",
    "    return shar, descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([np.sum([new_filter_0])])\n",
    "print(['Mono_same', 'Poly_same','Poly_mix', 'Poly_diff', 'Mono_diff'])\n",
    "print([ np.sum(  (descriptor_a==1)[new_filter_0]), np.sum(  (descriptor_a==2)[new_filter_0]),np.sum(  (descriptor_a==0)[new_filter_0]),np.sum(  (descriptor_a==3)[new_filter_0]),np.sum(  (descriptor_a==4)[new_filter_0])])\n",
    "print([ np.sum(  (descriptor_q==1)[new_filter_0]), np.sum(  (descriptor_q==2)[new_filter_0]),np.sum(  (descriptor_q==0)[new_filter_0]),np.sum(  (descriptor_q==3)[new_filter_0]),np.sum(  (descriptor_q==4)[new_filter_0])])\n",
    "print([ np.sum((descriptor_mer==1)[new_filter_0]), np.sum((descriptor_mer==2)[new_filter_0]),np.sum((descriptor_mer==0)[new_filter_0]),np.sum((descriptor_mer==3)[new_filter_0]),np.sum((descriptor_mer==4)[new_filter_0])])\n",
    "print([ np.sum((descriptor_mel==1)[new_filter_0]), np.sum((descriptor_mel==2)[new_filter_0]),np.sum((descriptor_mel==0)[new_filter_0]),np.sum((descriptor_mel==3)[new_filter_0]),np.sum((descriptor_mel==4)[new_filter_0])])\n",
    "print([ np.sum(  (descriptor_c==1)[new_filter_0]), np.sum(  (descriptor_c==2)[new_filter_0]),np.sum(  (descriptor_c==0)[new_filter_0]),np.sum(  (descriptor_c==3)[new_filter_0]),np.sum(  (descriptor_c==4)[new_filter_0])])\n",
    "print([ np.sum(  (descriptor_e==1)[new_filter_0]), np.sum(  (descriptor_e==2)[new_filter_0]),np.sum(  (descriptor_e==0)[new_filter_0]),np.sum(  (descriptor_e==3)[new_filter_0]),np.sum(  (descriptor_e==4)[new_filter_0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Creating the DAFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Function below has not been updated to use the new standardized data formats, and therefore would have to brushed up to be used in conjunction with current functions. __out1__ and __out2__ need to be changed to suit __pop_ac__ and __pop_alt__. \n",
    "\n",
    "__Method Used:__\n",
    "(A.)Â  Calculating an ancestral sequence\n",
    "\n",
    "If there is a match in the population with out1, write match to anc, else\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;If there is a match with out2, write match to anc, else\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If there s a match between out1 and out2, take as anc, else\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If no match, write ND1\n",
    "\n",
    "(B.) Counting the derived alleles\n",
    "\n",
    "If the ancestral has ND1, write ND2, else\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;If the population has only one non-match to a valid ancestral base, count the non-match as derived, else\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If the population has more than one non-matches, write ND3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the ancestral \n",
    "def build_ancestral(out1,out2):\n",
    "    anc = np.zeros((len(genome)),dtype = '|S1').astype('U')\n",
    "\n",
    "    anc[out1==REF]=out1[out1==REF]\n",
    "\n",
    "    anc[(ALT_1!='') & (out1==ALT_1)]=out1[(ALT_1!='') & (out1==ALT_1)]\n",
    "    anc[(ALT_2!='') & (out1==ALT_2)]=out1[(ALT_2!='') & (out1==ALT_2)]\n",
    "    anc[(ALT_3!='') & (out1==ALT_3)]=out1[(ALT_3!='') & (out1==ALT_3)]\n",
    "    anc[(anc=='') & (REF!='') & (out2==REF)]= out2[(anc=='') & (REF!='') & (out2==REF)]\n",
    "    anc[(anc=='') & (ALT_1!='') & (out2==ALT_1)]=out2[(anc=='') & (ALT_1!='') & (out2==ALT_1)]\n",
    "    anc[(anc=='') & (ALT_2!='') & (out2==ALT_2)]=out2[(anc=='') & (ALT_2!='') & (out2==ALT_2)]\n",
    "    anc[(anc=='') & (ALT_3!='') & (out2==ALT_3)]=out2[(anc=='') & (ALT_3!='') & (out2==ALT_3)]\n",
    "    anc[(anc=='') & (out1==out2)]=out1[(anc=='') & (out1==out2)]\n",
    "    anc[anc=='']='M'\n",
    "\n",
    "    return anc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_derived(anc):\n",
    "    der = np.zeros((len(genome),1),dtype = int).astype('U')\n",
    "    \n",
    "    der[REF!=anc]= ext_ac_0[REF!=anc]\n",
    "    der[(ALT_1!='') & (ALT_1!=anc)] = ext_ac_1[(ALT_1!='') & (ALT_1!=anc)]\n",
    "    #unnecessary if only choosing bi-allelic sites. \n",
    "    #der[(ALT_2!='') & (ALT_2!=anc)] = der[(ALT_2!='') & (ALT_2!=anc)] + ext_ac_2[(ALT_2!='') & (ALT_2!=anc)]\n",
    "    #der[(ALT_3!='') & (ALT_3!=anc)] = der[(ALT_3!='') & (ALT_3!=anc)] + ext_ac_3[(ALT_3!='') & (ALT_3!=anc)]\n",
    "\n",
    "    der[(ALT_2!='')] = -2\n",
    "    der[(REF!=anc) & (ALT_1!='') & (ALT_1!=anc)] = -2\n",
    "\n",
    "    der[anc=='M']=-1\n",
    "    # M = ND1 -1= ND2 -2= ND3\n",
    "\n",
    "    return der"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Creating the MAFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the MAFs only require accessing __pop_ac__ of a given population. \n",
    "<br>Allele counts were organized above by frequency at each site, so the second column, of the second highest frequency allele counts, forms the entirely of the MAFs. \n",
    "<br>An example MAF of four-fold sites of BFgam is given below.\n",
    "<br> The values are rerouted through the __create_MAFs__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t4 is equal to the total individuals of population 4, BFgam\n",
    "def create_MAFs(unique,counts, totinds):\n",
    "    counts_new = np.zeros(int((totinds/2)+1), dtype = int)\n",
    "    for i in range(0,len(unique)):\n",
    "        al = unique[i]\n",
    "        counts_new[al] = counts[i]       \n",
    "    return counts_new\n",
    "\n",
    "unique, counts = np.unique(BFg_ac[:,1][new_filter_4], return_counts=True)\n",
    "counts_new = create_MAFs(unique, counts, t4))\n",
    "list(counts_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 Building the Conserved, Non-conserved Sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conserved sites are defined as sites where the amino acid coded for by chri is the same as that coded for by epir. \n",
    "\n",
    "This therefore requires determining what amino acids are coded for by each species at each site. The following functions deal with that (and are an adaptation of Alistair's codon degeneracy code). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first cell, data tables and features are imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_features = etl.frompickle(os.path.join(output_dir, 'tbl_features.pkl.gz'))\n",
    "from Bio.Data import CodonTable\n",
    "standard_table = CodonTable.unambiguous_dna_by_name[\"Standard\"]\n",
    "bases = ['A', 'C', 'T', 'G']\n",
    "codon_table = standard_table.forward_table.copy()\n",
    "for codon in standard_table.stop_codons:\n",
    "    codon_table[codon] = '*'\n",
    "from Bio.Seq import Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, appropriate formats for the reference data of chri and epir are defined with __c_ref_aa__ and __e_ref_aa__. These will contain the amino acids coded for at each site.\n",
    "\n",
    "__chri_ref__ is created, which contains the outgroup nucleotide sequence. c_2L, c_2R,... c_X are loaded previously as shown in section 2. __epir_ref__ is similarly created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ref_aa = {k: np.zeros(len(genome[k]), dtype='U')\n",
    "                  for k in genome.keys()}\n",
    "e_ref_aa = {k: np.zeros(len(genome[k]), dtype='U')\n",
    "                  for k in genome.keys()}\n",
    "#load these separately \n",
    "chri_ref = {k: np.zeros(len(genome[k]), dtype='U')\n",
    "                  for k in genome.keys()}\n",
    "\n",
    "chri_ref['2L']  =  np.reshape( c_2L[:,0], len(e_2L)) \n",
    "chri_ref['2R']  =  np.reshape( c_2R[:,0], len(e_2R)) \n",
    "chri_ref['3L']  =  np.reshape( c_3L[:,0], len(e_3L)) \n",
    "chri_ref['3R']  =  np.reshape( c_3R[:,0], len(e_3R)) \n",
    "chri_ref['X']   =  np.reshape( c_X[:,0], len(e_X)) \n",
    "chri_ref['Y_unplaced'][:]   =  'N'\n",
    "chri_ref['UNKN'][:]   =  'N'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below builds __c_ref_aa__ by using the codon position and phase data for the coding section sequences, and consulting the codon table. To build __e_ref_aa__ change __chri_ref__ to __epir_ref__ (built equally as above), and __c_ref_aa__ to __e_ref_aa__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cds in tbl_features.eq('type', 'CDS').true('is_canonical').records():\n",
    "    if cds.seqid not in genome:\n",
    "        continue\n",
    "    if cds.strand == '+':\n",
    "        seq = ''.join(chri_ref[cds.seqid][cds.start:cds.stop])\n",
    "        phase = int(cds.phase)\n",
    "        for i in range(cds.length):\n",
    "            codon_pos = (i - phase) % 3\n",
    "            if codon_pos == 0:\n",
    "                codon_start = i\n",
    "                codon_stop = i+3\n",
    "            elif codon_pos == 1:\n",
    "                codon_start = i-1\n",
    "                codon_stop = i+2\n",
    "            elif codon_pos == 2:\n",
    "                codon_start = i-2\n",
    "                codon_stop = i+1\n",
    "            else:\n",
    "                raise RuntimeError('unexpected codon_pos', codon_pos)\n",
    "            if codon_start < 0:\n",
    "                continue\n",
    "            if codon_stop > cds.length:\n",
    "                continue\n",
    "            codon = seq[codon_start:codon_stop]\n",
    "            if 'N' not in codon:\n",
    "                ref_aa = codon_table[codon]\n",
    "                c_ref_aa[cds.seqid][cds.start + i] = ref_aa\n",
    "\n",
    "    else:\n",
    "        seq = ''.join(chri_ref[cds.seqid][cds.start:cds.stop]).upper()\n",
    "        seq = str(Seq(seq).reverse_complement())\n",
    "        phase = int(cds.phase)\n",
    "        for i in range(cds.length):\n",
    "            codon_pos = (i - phase) % 3\n",
    "            if codon_pos == 0:\n",
    "                codon_start = i\n",
    "                codon_stop = i+3\n",
    "            elif codon_pos == 1:\n",
    "                codon_start = i-1\n",
    "                codon_stop = i+2\n",
    "            elif codon_pos == 2:\n",
    "                codon_start = i-2\n",
    "                codon_stop = i+1\n",
    "            else:\n",
    "                raise RuntimeError('unexpected codon_pos', codon_pos)\n",
    "            if codon_start < 0:\n",
    "                continue\n",
    "            if codon_stop > cds.length:\n",
    "                continue\n",
    "            codon = seq[codon_start:codon_stop]\n",
    "            if 'N' not in codon:\n",
    "                ref_aa = codon_table[codon]\n",
    "                alt_aa = [a for a in aas if a != ref_aa]\n",
    "                nonsyn = len(alt_aa)\n",
    "                seq_index = cds.stop - i - 1\n",
    "                c_ref_aa[cds.seqid][seq_index] = ref_aa\n",
    "                \n",
    "#                 print(i, codon_pos, codon, codons, ref_aa, aas, deg, nonsyn, seq_index, genome[cds.seqid][seq_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below builds __con_all__ and __ncon_all__, the arrays that define the filters for all conserved and non-conserved sites in the coding regions of the sequences. It compares __c_ref_aa__ to __e_ref_aa__. Sites that do not code for an amino acid are not included in either __con_all__ or __ncon_all__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare c_ref_aa with e_ref_aa\n",
    "con_all = {k: np.zeros(len(genome[k]), dtype= int)\n",
    "                  for k in genome.keys()}\n",
    "ncon_all = {k: np.zeros(len(genome[k]), dtype= int)\n",
    "                  for k in genome.keys()}\n",
    "\n",
    "c1_2L = c_ref_aa['2L'][:]\n",
    "c1_2R = c_ref_aa['2R'][:]\n",
    "c1_3L = c_ref_aa['3L'][:]\n",
    "c1_3R = c_ref_aa['3R'][:]\n",
    "c1_X  = c_ref_aa['X'][:]\n",
    "\n",
    "e1_2L = e_ref_aa['2L'][:]\n",
    "e1_2R = e_ref_aa['2R'][:]\n",
    "e1_3L = e_ref_aa['3L'][:]\n",
    "e1_3R = e_ref_aa['3R'][:]\n",
    "e1_X  = e_ref_aa['X'][:]\n",
    "\n",
    "con_all['2L'] = (c1_2L!='') & (e1_2L!='') & (c1_2L == e1_2L)\n",
    "con_all['2R'] = (c1_2R!='') & (e1_2R!='') & (c1_2R == e1_2R)\n",
    "con_all['3L'] = (c1_3L!='') & (e1_3L!='') & (c1_3L == e1_3L)\n",
    "con_all['3R'] = (c1_3R!='') & (e1_3R!='') & (c1_3R == e1_3R)\n",
    "con_all['X']  = (c1_X!='')  & (e1_X!='')  & (c1_X == e1_X)\n",
    "\n",
    "ncon_all['2L'] = (c1_2L!='') & (e1_2L!='') & (c1_2L != e1_2L)\n",
    "ncon_all['2R'] = (c1_2R!='') & (e1_2R!='') & (c1_2R != e1_2R)\n",
    "ncon_all['3L'] = (c1_3L!='') & (e1_3L!='') & (c1_3L != e1_3L)\n",
    "ncon_all['3R'] = (c1_3R!='') & (e1_3R!='') & (c1_3R != e1_3R)\n",
    "ncon_all['X']  = (c1_X!='')  & (e1_X!='')  & (c1_X  != e1_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the filters, it is suggested to run this once, save as a zarr group, and access it as such in other files/ other moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving as a zarr group\n",
    "zarr.save_group(os.path.join(output_dir, 'chri_codons.zarr.zip'), \n",
    "                **c_ref_all)\n",
    "zarr.save_group(os.path.join(output_dir, 'epir_codons.zarr.zip'), \n",
    "                **e_ref_all)\n",
    "\n",
    "#Accessing zarr group\n",
    "con_all = zarr.open_group(str(output_dir / 'con_all.zarr.zip'))\n",
    "ncon_all = zarr.open_group(str(output_dir / 'ncon_all.zarr.zip'))\n",
    "con = con_all[seqid][:]\n",
    "ncon = ncon_all[seqid][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter by PEST REFERENCE amino acids, repeat cells (1,2,3) with the genome reference data rather than __chri_ref__ or __epir_ref__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the degeneracies of chri and epir, insert the sections below in cell 3 of this section as appropriate. Also save and access as a zarr group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting degeneracies modificiation!\n",
    "#insert section below before the first 'ref_aa =...'\n",
    "                codons = None\n",
    "                if codon_pos == 0:\n",
    "                    codons = [b + codon[1:] for b in bases]\n",
    "                elif codon_pos == 1:\n",
    "                    codons = [codon[0] + b + codon[2] for b in bases]\n",
    "                elif codon_pos == 2:\n",
    "                    codons = [codon[:2] + b for b in bases]\n",
    "                aas = [codon_table[c] for c in codons]\n",
    "                # assign degeneracy\n",
    "                aas_ct = collections.Counter(aas)\n",
    "                if len(aas_ct) == 1:\n",
    "                    # 4-fold degenerate\n",
    "                    deg = 4\n",
    "                elif len(aas_ct) == 4:\n",
    "                    # 0-fold degenerate\n",
    "                    deg = 1\n",
    "                elif len(aas_ct) == 2 and tuple(aas_ct.values()) == (2, 2):\n",
    "                    # simple 2-fold degeneracy\n",
    "                    deg = 2\n",
    "                else:\n",
    "                    # complex 2-fold degenerate\n",
    "                    deg = 3\n",
    "                # assign number of nonsyn mutations\n",
    "                codon_degeneracy[cds.seqid][cds.start + i] = deg\n",
    "                \n",
    "\n",
    "#insert section below before the second 'ref_aa =...'\n",
    "                codons = None\n",
    "                if codon_pos == 0:\n",
    "                    codons = [b + codon[1:] for b in bases]\n",
    "                elif codon_pos == 1:\n",
    "                    codons = [codon[0] + b + codon[2] for b in bases]\n",
    "                elif codon_pos == 2:\n",
    "                    codons = [codon[:2] + b for b in bases]\n",
    "                aas = [codon_table[c] for c in codons]\n",
    "                # assign degeneracy\n",
    "                aas_ct = collections.Counter(aas)\n",
    "                if len(aas_ct) == 1:\n",
    "                    # 4-fold degenerate\n",
    "                    deg = 4\n",
    "                elif len(aas_ct) == 4:\n",
    "                    # 0-fold degenerate\n",
    "                    deg = 1\n",
    "                elif len(aas_ct) == 2 and tuple(aas_ct.values()) == (2, 2):\n",
    "                    # simple 2-fold degenerate\n",
    "                    deg = 2\n",
    "                else:\n",
    "                    # complex 2-fold degenerate\n",
    "                    deg = 3\n",
    "                # assign number of nonsyn mutations\n",
    "                codon_degeneracy[cds.seqid][seq_index] = deg              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Selecting Genes\n",
    "\n",
    "A filter is created below that filters for the coding sections of relevant genes in a particular chromosome. \n",
    "<br> The filter takes as input the __seqid__ of the particular chromosome (eg: '2L'), and the array __selected_genes__ that contains the names of the genes of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example array of relevant genes\n",
    "selected_genes = ['AGAP004677', 'AGAP004678','AGAP004688','AGAP004698','AGAP004718']\n",
    "\n",
    "def filter_genes( selected_genes, seqid):\n",
    "    tbl_features = etl.frompickle(os.path.join(output_dir, 'tbl_features.pkl.gz'))\n",
    "    \n",
    "    start_stop = np.zeros((53131,2), dtype = int)\n",
    "    for mrna in tbl_features.eq('type', 'mRNA').true('is_canonical').records():\n",
    "        if mrna.parent in selected_genes:\n",
    "            for cds in tbl_features.eq('type', 'CDS').true('is_canonical').records():\n",
    "                if cds.parent==mrna.ID:\n",
    "                    start_stop[i,0] = cds.start\n",
    "                    start_stop[i,1] = cds.stop\n",
    "    start_stop = start_stop[0: (np.sum(start_stop[:,0]!=0))]\n",
    "    \n",
    "    genome = phase2_ar1.genome_agamp3[seqid][:]\n",
    "    gene_filter = np.zeros(len(genome), dtype = int)\n",
    "    for i in range (0,len(ss)):\n",
    "        start = start_stop[i,0]\n",
    "        stop = start_stop[i,1]\n",
    "        gene_filter[start:stop]=1\n",
    "    \n",
    "    return gene_filter\n",
    "\n",
    "* OUTPUT ALL THE CHROMOSOMES INSTEAD * \n",
    "* DO MAPPING GENES *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 Visualizing Data\n",
    "\n",
    "The pandas module can be used to customize tables with which to visualize data.\n",
    "<br> The table below shows all sites that are filtered for with Filter 2.0. The columns show the alleles of populations (BFg, BFc, KE) and outgroups (arab, quad, meru, mela, chri, epir). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df():\n",
    "    df = pandas.DataFrame.from_items([\n",
    "        ('POS', np.arange(1,len(genome)+1)[new_filter_tot]),\n",
    "        ('Deg', codon_degeneracy[new_filter_tot]),\n",
    "        ('BFg', BFg_alt[:,0][new_filter_tot]),\n",
    "        #('BFg', BFg_ac [:,0][new_filter_tot]),\n",
    "        ('BFg', BFg_alt[:,1][new_filter_tot]),\n",
    "        #('BFg', BFg_ac [:,1][new_filter_tot]),\n",
    "        ('BFc', BFc_alt[:,0][new_filter_tot]),\n",
    "        #('BFc', BFc_ac [:,0][new_filter_tot]),\n",
    "        ('BFc', BFc_alt[:,1][new_filter_tot]),\n",
    "        #('BFc', BFc_ac [:,1][new_filter_tot]),\n",
    "        ('KE' , KE_alt [:,0][new_filter_tot]),\n",
    "        #('KE' , KE_ac  [:,0][new_filter_tot]),\n",
    "        ('KE' , KE_alt [:,1][new_filter_tot]),\n",
    "        #('KE' , KE_ac  [:,1][new_filter_tot]),\n",
    "        ('A', a_alt  [:,0][new_filter_tot]),\n",
    "        #('A', a_ac   [:,0][new_filter_tot]),\n",
    "        ('A', a_alt  [:,1][new_filter_tot]),\n",
    "        #('A', a_ac   [:,1][new_filter_tot]),\n",
    "        ('Q', q_alt  [:,0][new_filter_tot]),\n",
    "        #('Q', q_ac   [:,0][new_filter_tot]),\n",
    "        ('Q', q_alt  [:,1][new_filter_tot]),\n",
    "        #('Q', q_ac   [:,1][new_filter_tot]),\n",
    "        ('Mer', mer_alt[:,0][new_filter_tot]),\n",
    "        #('Mer', mer_ac [:,0][new_filter_tot]),\n",
    "        ('Mer', mer_alt[:,1][new_filter_tot]),\n",
    "        #('Mer', mer_ac [:,1][new_filter_tot]),\n",
    "        ('Mel', mel_alt[:,0][new_filter_tot]),\n",
    "        #('Mel', mel_ac [:,0][new_filter_tot]),\n",
    "        ('Mel', mel_alt[:,1][new_filter_tot]),\n",
    "        #('Mel', mel_ac [:,1][new_filter_tot]),\n",
    "        ('C', c_alt  [:,0][new_filter_tot]),\n",
    "        ('E', e_alt  [:,0][new_filter_tot]),\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "build_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
